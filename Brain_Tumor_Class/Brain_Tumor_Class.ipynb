{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257e2e0b-1d77-4512-b708-6aa3b4a1310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 16:15:25.541539: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c46d010-4dff-4de7-bb16-543c740850b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset directory (ensure the folder is in your working directory)\n",
    "dataset_dir = \"data/Training/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da4fa9-c432-4569-ae5e-624e0b21c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration\n",
    "\n",
    "# List all class folders (each folder represents a tumor type)\n",
    "classes = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "print(\"Classes found:\", classes)\n",
    "\n",
    "# Initialize dictionary to store image counts per class\n",
    "class_counts = defaultdict(int)\n",
    "\n",
    "# Loop through each class folder, count and display the number of images\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(dataset_dir, cls)\n",
    "    image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    class_counts[cls] = len(image_files)\n",
    "    print(f\"Class '{cls}' has {len(image_files)} images.\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')\n",
    "plt.title(\"Class Distribution in Brain Tumor MRI Dataset\")\n",
    "plt.xlabel(\"Tumor Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd337014-73c9-4e72-9b20-f4426a2048b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    try:\n",
    "        # Read image using OpenCV (BGR format)\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(\"Image not loaded properly.\")\n",
    "        # Convert from BGR to RGB format for display\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # Resize image to target size\n",
    "        img = cv2.resize(img, target_size)\n",
    "        # Normalize pixel values to [0, 1]\n",
    "        img = img / 255.0\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load and display one sample image from each class\n",
    "sample_images = {}\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(dataset_dir, cls)\n",
    "    image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if image_files:\n",
    "        sample_path = os.path.join(class_path, image_files[0])\n",
    "        img = load_and_preprocess_image(sample_path)\n",
    "        if img is not None:\n",
    "            sample_images[cls] = img\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for idx, (cls, img) in enumerate(sample_images.items()):\n",
    "    plt.subplot(1, len(sample_images), idx+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(cls)\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"Sample Preprocessed Images from Each Tumor Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9642e237-6420-4f60-9896-015ea64d8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation using Keras ImageDataGenerator\n",
    "\n",
    "# Define augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "# Example: Augment a sample image from one class\n",
    "if sample_images:\n",
    "    sample_class = list(sample_images.keys())[0]\n",
    "    img = sample_images[sample_class]\n",
    "    img_expanded = np.expand_dims(img, axis=0)  # Expand dims for generator\n",
    "    aug_iter = datagen.flow(img_expanded, batch_size=1)\n",
    "    aug_images = [next(aug_iter)[0] for _ in range(5)]\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, aug_img in enumerate(aug_images):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        plt.imshow(aug_img)\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"Data Augmentation Examples for Class '{sample_class}'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af690609-54a5-4b27-98cf-34bbccd0c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "\n",
    "# Basic CNN Model\n",
    "def create_basic_cnn(input_shape=(224,224,3), num_classes=4):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "basic_cnn = create_basic_cnn()\n",
    "basic_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "basic_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76855c-197b-44c8-8e19-12973e8dd2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced CNN Model with Regularization\n",
    "def create_enhanced_cnn(input_shape=(224,224,3), num_classes=4):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Conv2D(128, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2,2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "enhanced_cnn = create_enhanced_cnn()\n",
    "enhanced_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "enhanced_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9196b-edf7-40bc-9350-093e303b7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning Model\n",
    "def create_transfer_learning_model(input_shape=(224,224,3), num_classes=4):\n",
    "    # Load pre-trained VGG16 model without the top classifier layers\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "transfer_model = create_transfer_learning_model()\n",
    "transfer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0215387-05ca-4e08-b98c-5d91032e0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation\n",
    "\n",
    "# Define image dimensions and batch size\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "# Set the directory where the training data is stored (adjust this path as needed)\n",
    "train_dir = \"data/Training/\"\n",
    "test_dir = \"data/Testing/\"\n",
    "\n",
    "# Create an ImageDataGenerator with augmentation and a validation split for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,  # Reserve 20% for validation\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Generate training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Generate validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Create a separate ImageDataGenerator for the test set (only rescaling, no augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generate test data from the hold-out Testing directory\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "epochs = 10  # Adjust epochs as needed\n",
    "\n",
    "# Function to train and evaluate a given model on training and validation sets\n",
    "def train_and_evaluate(model, model_name):\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // batch_size,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.samples // batch_size,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_acc = model.evaluate(validation_generator, steps=validation_generator.samples // batch_size)\n",
    "    print(f\"{model_name} Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"{model_name} Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Generate predictions for a classification report on validation set\n",
    "    validation_generator.reset()\n",
    "    preds = model.predict(validation_generator, steps=validation_generator.samples // batch_size, verbose=1)\n",
    "    predicted_class_indices = np.argmax(preds, axis=1)\n",
    "    true_class_indices = validation_generator.classes[:len(predicted_class_indices)]\n",
    "    \n",
    "    print(f\"\\nClassification Report for {model_name} (Validation):\")\n",
    "    target_names = list(validation_generator.class_indices.keys())\n",
    "    print(classification_report(true_class_indices, predicted_class_indices, target_names=target_names))\n",
    "    \n",
    "    return history, val_loss, val_acc\n",
    "\n",
    "# Function to evaluate a given model on the hold-out test set\n",
    "def evaluate_on_test(model, model_name):\n",
    "    print(f\"\\nEvaluating {model_name} on hold-out Test dataset...\")\n",
    "    test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
    "    print(f\"{model_name} Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"{model_name} Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Generate predictions for a classification report on test set\n",
    "    test_generator.reset()\n",
    "    preds = model.predict(test_generator, steps=test_generator.samples // batch_size, verbose=1)\n",
    "    predicted_class_indices = np.argmax(preds, axis=1)\n",
    "    true_class_indices = test_generator.classes[:len(predicted_class_indices)]\n",
    "    \n",
    "    print(f\"\\nClassification Report for {model_name} (Test):\")\n",
    "    target_names = list(test_generator.class_indices.keys())\n",
    "    print(classification_report(true_class_indices, predicted_class_indices, target_names=target_names))\n",
    "    \n",
    "    return test_loss, test_acc\n",
    "\n",
    "# Train and evaluate each model variation on training/validation, then test on the hold-out set\n",
    "\n",
    "# Basic CNN Model Evaluation\n",
    "print(\"### Basic CNN Model Evaluation ###\")\n",
    "history_basic, loss_basic, acc_basic = train_and_evaluate(basic_cnn, \"Basic CNN Model\")\n",
    "test_loss_basic, test_acc_basic = evaluate_on_test(basic_cnn, \"Basic CNN Model\")\n",
    "\n",
    "# Enhanced CNN Model Evaluation\n",
    "print(\"\\n### Enhanced CNN Model Evaluation ###\")\n",
    "history_enhanced, loss_enhanced, acc_enhanced = train_and_evaluate(enhanced_cnn, \"Enhanced CNN Model\")\n",
    "test_loss_enhanced, test_acc_enhanced = evaluate_on_test(enhanced_cnn, \"Enhanced CNN Model\")\n",
    "\n",
    "# Transfer Learning Model Evaluation\n",
    "print(\"\\n### Transfer Learning Model Evaluation ###\")\n",
    "history_transfer, loss_transfer, acc_transfer = train_and_evaluate(transfer_model, \"Transfer Learning Model\")\n",
    "test_loss_transfer, test_acc_transfer = evaluate_on_test(transfer_model, \"Transfer Learning Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c1f08-5002-42c5-b6e6-ed80018de3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "\n",
    "# Define model names and their corresponding performance metrics\n",
    "models = ['Basic CNN', 'Enhanced CNN', 'Transfer Learning']\n",
    "test_accuracies = [test_acc_basic, test_acc_enhanced, test_acc_transfer]\n",
    "test_losses = [test_loss_basic, test_loss_enhanced, test_loss_transfer]\n",
    "\n",
    "# Create a figure with two subplots: one for accuracy and one for loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Test Accuracy Comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(models, test_accuracies, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title(\"Test Accuracy Comparison\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)  # Accuracy ranges from 0 to 1\n",
    "\n",
    "# Plot Test Loss Comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(models, test_losses, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title(\"Test Loss Comparison\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0, max(test_losses)*1.2)  # Extend y-axis a bit above the maximum loss\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
